{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "sys.path.append(os.path.abspath((\"../tools/\")))\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, load_classifier_and_data, test_classifier\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_project_dataset.pkl', 'rb') as src:\n",
    "    data_dict = pickle.load(src)\n",
    "\n",
    "feature_set = set()\n",
    "\n",
    "# find all unique feature\n",
    "data_items = data_dict.items()\n",
    "for employee, features in data_items:\n",
    "    for feature in features:\n",
    "        feature_set.add(feature)\n",
    "\n",
    "# validate that all records have those features.\n",
    "for employee, features in data_items:\n",
    "    contains_all_features = all([f in feature_set for f in features.keys()])\n",
    "    if not contains_all_features:\n",
    "        raise KeyError(\"features missing...\")\n",
    "\n",
    "feature_set.remove('poi')\n",
    "feature_set.remove('email_address') # feature will not work in featureFormat\n",
    "\n",
    "# Create new features\n",
    "for emp in data_dict:\n",
    "    if not data_dict[emp]['from_poi_to_this_person'] == 'NaN' \\\n",
    "        and data_dict[emp]['from_this_person_to_poi'] == 'NaN':\n",
    "        data_dict[emp]['from_ratio'] = data_dict[emp]['from_poi_to_this_person'] / data_dict[emp]['to_messages']\n",
    "        data_dict[emp]['to_ratio'] = data_dict[emp]['from_this_person_to_poi'] / data_dict[emp]['from_messages']\n",
    "    else:\n",
    "        data_dict[emp]['from_ratio'] = 'NaN'\n",
    "        data_dict[emp]['to_ratio'] = 'NaN'\n",
    "\n",
    "feature_set.add('from_ratio')\n",
    "feature_set.add('to_ratio')\n",
    "set_list = list(feature_set)\n",
    "set_list.sort()\n",
    "features_list = ['poi'] + set_list\n",
    "\n",
    "# Remove some meaningless entries\n",
    "for name in ('TOTAL', 'THE TRAVEL AGENCY IN THE PARK'):\n",
    "    data_dict.pop(name)\n",
    "\n",
    "# Format data for ML activities\n",
    "def format_and_split(data_dict, features_list):\n",
    "    return targetFeatureSplit(\n",
    "        featureFormat(\n",
    "            data_dict,\n",
    "            features_list\n",
    "        )\n",
    "    )\n",
    "labels, features = format_and_split(data_dict, features_list)\n",
    "labels, features = np.array(labels), np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_extractor(best_params_, estimator_name):\n",
    "    \"\"\" \n",
    "    param_extractor extracts and formats the best hyper-parameters\n",
    "    from the best_params_ attribute of GridSearchCV\n",
    "    \"\"\"\n",
    "\n",
    "    if not estimator_name.endswith(\"__\"):\n",
    "        estimator_name = estimator_name + \"__\"\n",
    "    \n",
    "    return {\n",
    "    k.replace(estimator_name, \"\"): v\n",
    "    for k, v in best_params_.items()\n",
    "    if k.startswith(estimator_name)\n",
    "    }\n",
    "\n",
    "def scorer(test_labels, test_predictions):\n",
    "    \"\"\"\n",
    "    scorer generates accuracy, precision, and recall\n",
    "    metrics for a given model\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        accuracy=accuracy_score(test_labels, test_predictions),\n",
    "        precision=precision_score(test_labels, test_predictions),\n",
    "        recall=recall_score(test_labels, test_predictions),\n",
    "    )\n",
    "\n",
    "def best_features_extractor(selector, features_list):\n",
    "    best_features = []\n",
    "    for kbest in selector.get_support(indices=True):\n",
    "        best_features.append(features_list[1:][kbest])\n",
    "    return best_features\n",
    "\n",
    "def set_estimator_params(pipeline, best_params_):\n",
    "    # Update selector params\n",
    "    pipeline.steps[1][1].set_params(**param_extractor(best_params_, 'selector'))\n",
    "    pipeline.steps[2][1].set_params(**param_extractor(best_params_, 'classifier'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating various classifiers.\n",
      "('Estimator: ', 'GaussianNB', 'scores: ', {'recall': 0.2800000000000001, 'precision': 0.5166666666666667, 'accuracy': 0.8611111111111113})\n",
      "('Estimator: ', 'RandomForestClassifier', 'scores: ', {'recall': 0.31999999999999995, 'precision': 0.4208333333333334, 'accuracy': 0.7833333333333333})\n",
      "('Estimator: ', 'KNeighborsClassifier', 'scores: ', {'recall': 0.34, 'precision': 0.3047916666666667, 'accuracy': 0.761111111111111})\n",
      "('Estimator: ', 'DecisionTreeClassifier', 'scores: ', {'recall': 0.32, 'precision': 0.3191666666666667, 'accuracy': 0.7805555555555554})\n",
      "('Estimator: ', 'AdaBoostClassifier', 'scores: ', {'recall': 0.36000000000000004, 'precision': 0.31507936507936507, 'accuracy': 0.7722222222222221})\n",
      "Performing synthetic tests with stratified sampling.\n",
      "('GaussianNB', {'recall': 0.2219999999999998, 'precision': 0.4461904761904762, 'accuracy': 0.8591666666666663}, ['bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value'])\n",
      "('RandomForestClassifier', {'recall': 0.09799999999999999, 'precision': 0.23792857142857138, 'accuracy': 0.8438888888888895}, ['bonus', 'expenses', 'from_poi_to_this_person', 'from_ratio', 'from_this_person_to_poi', 'other', 'salary', 'shared_receipt_with_poi', 'total_stock_value'])\n",
      "('KNeighborsClassifier', {'recall': 0.12199999999999996, 'precision': 0.37, 'accuracy': 0.8630555555555559}, ['bonus', 'exercised_stock_options', 'salary', 'shared_receipt_with_poi', 'total_stock_value'])\n",
      "('DecisionTreeClassifier', {'recall': 0.22999999999999976, 'precision': 0.3632857142857142, 'accuracy': 0.8388888888888891}, ['exercised_stock_options', 'salary', 'total_stock_value'])\n",
      "('AdaBoostClassifier', {'recall': 0.2479999999999998, 'precision': 0.35396428571428573, 'accuracy': 0.834166666666667}, ['bonus', 'exercised_stock_options', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'total_payments', 'total_stock_value'])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler() # Changed from StandardScaler, MinMax \n",
    "# I dont think we can reuse this...\n",
    "# selector = SelectKBest() # k, score_func\n",
    "\n",
    "gaussian = GaussianNB() # priors, var_smoothing\n",
    "random_forest = RandomForestClassifier() # n_estimators, \n",
    "neighbors = KNeighborsClassifier()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "ada_boost = AdaBoostClassifier()\n",
    "\n",
    "estimators = [{\n",
    "\t'name': \"GaussianNB\",\n",
    "\t'classifier': Pipeline([\n",
    "\t\t('scaler', scaler),\n",
    "\t\t('selector', SelectKBest()),\n",
    "\t\t('classifier', gaussian)\n",
    "\t]),\n",
    "\t'param_grid': {\n",
    "\t\t\"selector__k\": [5, 7, 9],\n",
    "\t\t\"classifier__var_smoothing\": np.logspace(0,-9, num=20),\n",
    "\t}\n",
    "}, {\n",
    "\t'name': 'RandomForestClassifier',\n",
    "\t'classifier': Pipeline([\n",
    "\t\t('scaler', scaler),\n",
    "\t\t('selector', SelectKBest()),\n",
    "\t\t('classifier', random_forest)\n",
    "\t]),\n",
    "\t'param_grid': {\n",
    "\t\t\"selector__k\": [5, 7, 9],\n",
    "\t\t\"selector__score_func\": [f_classif, mutual_info_classif, chi2],\n",
    "\t\t\"classifier__n_estimators\": [110, 115, 120, 125],\n",
    "\t\t\"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "\t\t\"classifier__min_samples_split\": [2, 3, 5],\n",
    "\t}\n",
    "}, {\n",
    "\t'name': 'KNeighborsClassifier',\n",
    "\t'classifier': Pipeline([\n",
    "\t\t('scaler', scaler),\n",
    "\t\t('selector', SelectKBest()),\n",
    "\t\t('classifier', neighbors)\n",
    "\t]),\n",
    "\t'param_grid': {\n",
    "\t\t\"selector__k\": [5, 7, 9],\n",
    "\t\t\"selector__score_func\": [f_classif, mutual_info_classif, chi2],\n",
    "\t\t\"classifier__n_neighbors\": [3, 5, 7, 9],\n",
    "\t\t\"classifier__weights\": ['uniform', 'distance'],\n",
    "\t\t\"classifier__algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "\t\t\"classifier__metric\": ['minkowski', 'cityblock', 'euclidean'],\n",
    "\t}\n",
    "}, {\n",
    "\t'name': 'DecisionTreeClassifier',\n",
    "\t'classifier': Pipeline([\n",
    "\t\t('scaler', scaler),\n",
    "\t\t('selector', SelectKBest()),\n",
    "\t\t('classifier', decision_tree)\n",
    "\t]),\n",
    "\t'param_grid': {\n",
    "\t\t\"selector__k\": [3, 5, 7, 9],\n",
    "\t\t\"selector__score_func\": [f_classif, mutual_info_classif, chi2],\n",
    "\t\t\"classifier__max_leaf_nodes\": [2, 3, 5, 8, 13],\n",
    "\t\t\"classifier__min_samples_split\": [2, 3, 4, 5, 6, 7],\n",
    "\t}\n",
    "}, {\n",
    "\t'name': 'AdaBoostClassifier',\n",
    "\t'classifier': Pipeline([\n",
    "\t\t('scaler', scaler),\n",
    "\t\t('selector', SelectKBest()),\n",
    "\t\t('classifier', ada_boost)\n",
    "\t]),\n",
    "\t'param_grid': {\n",
    "\t\t\"selector__k\": [3, 5, 7, 9],\n",
    "\t\t\"selector__score_func\": [f_classif, mutual_info_classif, chi2],\n",
    "\t\t\"classifier__n_estimators\": [50, 63, 75, 86, 100],\n",
    "\t\t\"classifier__learning_rate\": [0.50, 0.75, 1, 1.25, 1.5],\n",
    "\t}\n",
    "}]\n",
    "\n",
    "# {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "\n",
    "# Evaluate various classifiers\n",
    "print(\"Evaluating various classifiers.\")\n",
    "for index, estimator in enumerate(estimators):\n",
    "\tclassifier = estimator['classifier']\n",
    "\tgrid = estimator['param_grid']\n",
    "\tn_splits = 10\n",
    "\tsplitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25).split\n",
    "\tscores = {\n",
    "\t\t'accuracy': [],\n",
    "\t\t'precision': [],\n",
    "\t\t'recall': [],\n",
    "\t}\n",
    "\tfor train_indices, test_indices in splitter(features, labels):\n",
    "\t\ttrain_features, test_features = features[train_indices], features[test_indices]\n",
    "\t\ttrain_labels, test_labels = labels[train_indices], labels[test_indices]\n",
    "\t\tpipe.fit(train_features, train_labels)\n",
    "\t\tscore = scorer(test_labels, pipe.predict(test_features))\n",
    "\t\tscores['accuracy'].append(score['accuracy'])\n",
    "\t\tscores['precision'].append(score['precision'])\n",
    "\t\tscores['recall'].append(score['recall'])\n",
    "\tmean_scores = {\n",
    "\t\t'accuracy': sum(scores['accuracy']) / n_splits,\n",
    "\t\t'precision': sum(scores['precision']) / n_splits,\n",
    "\t\t'recall': sum(scores['recall']) / n_splits,\n",
    "\t}\n",
    "\testimators[index].update({\n",
    "\t\t'mean_scores': mean_scores\n",
    "\t})\n",
    "\tparam_search = GridSearchCV(classifier, param_grid=grid)\n",
    "\tbest_params_ = param_search.fit(train_features, train_labels).best_params_\n",
    "\testimators[index].update({\n",
    "\t\t\"best_params_\": best_params_\n",
    "\t})\n",
    "\testimators[index].update({\n",
    "\t\t'best_estimator_': param_search.best_estimator_\n",
    "\t})\n",
    "\testimators[index].update({\n",
    "\t\t'best_features': best_features_extractor(param_search.best_estimator_.steps[1][1], features_list),\n",
    "\t\t'best_features_mask': param_search.best_estimator_.steps[1][1].get_support() \n",
    "\t})\n",
    "\tprint(\"Estimator: \", estimator['name'], \"scores: \", mean_scores)\n",
    "\n",
    "print(\"Performing synthetic tests with stratified sampling.\")\n",
    "for index, estimator in enumerate(estimators):\n",
    "    pipeline = estimator['best_estimator_']\n",
    "    scores = {\n",
    "      'accuracy': [],\n",
    "      'precision': [],\n",
    "      'recall': [],\n",
    "    }\n",
    "    n_splits = 100\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25).split\n",
    "    for train_indices, test_indices in splitter(features, labels):\n",
    "        train_features, test_features = features[train_indices], features[test_indices]\n",
    "        train_labels, test_labels = labels[train_indices], labels[test_indices]\n",
    "        pipeline.fit(train_features, train_labels)\n",
    "        score = scorer(test_labels, pipeline.predict(test_features))\n",
    "        scores['accuracy'].append(score['accuracy'])\n",
    "        scores['precision'].append(score['precision'])\n",
    "        scores['recall'].append(score['recall'])\n",
    "    mean_scores = {\n",
    "      'accuracy': sum(scores['accuracy']) / n_splits,\n",
    "      'precision': sum(scores['precision']) / n_splits,\n",
    "      'recall': sum(scores['recall']) / n_splits,\n",
    "\t  }\n",
    "    print(estimator['name'], mean_scores, estimator['best_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=5, score_func=<function f_classif at 0x7feb5dd634d0>)), ('classifier', GaussianNB(priors=None, var_smoothing=1.0))])\n",
      "\tAccuracy: 0.87571\tPrecision: 0.62264\tRecall: 0.33000\tF1: 0.43137\tF2: 0.36424\n",
      "\tTotal predictions: 14000\tTrue positives:  660\tFalse positives:  400\tFalse negatives: 1340\tTrue negatives: 11600\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=9, score_func=<function mutual_info_classif at 0x7feb5d3fedd0>)), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', m...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.84200\tPrecision: 0.33540\tRecall: 0.10800\tF1: 0.16339\tF2: 0.12494\n",
      "\tTotal predictions: 14000\tTrue positives:  216\tFalse positives:  428\tFalse negatives: 1784\tTrue negatives: 11572\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=5, score_func=<function chi2 at 0x7feb5dd63bd0>)), ('classifier', KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='cityblock',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.84950\tPrecision: 0.36591\tRecall: 0.07300\tF1: 0.12172\tF2: 0.08692\n",
      "\tTotal predictions: 14000\tTrue positives:  146\tFalse positives:  253\tFalse negatives: 1854\tTrue negatives: 11747\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=3, score_func=<function f_classif at 0x7feb5dd634d0>)), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=5, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.84169\tPrecision: 0.46539\tRecall: 0.19500\tF1: 0.27484\tF2: 0.22064\n",
      "\tTotal predictions: 13000\tTrue positives:  390\tFalse positives:  448\tFalse negatives: 1610\tTrue negatives: 10552\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=7, score_func=<function chi2 at 0x7feb5dd63bd0>)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.25, n_estimators=63, random_state=None))])\n",
      "\tAccuracy: 0.83247\tPrecision: 0.33918\tRecall: 0.27050\tF1: 0.30097\tF2: 0.28192\n",
      "\tTotal predictions: 15000\tTrue positives:  541\tFalse positives: 1054\tFalse negatives: 1459\tTrue negatives: 11946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for estimator in estimators:\n",
    "    classifier = estimator['best_estimator_']\n",
    "    best_features = estimator['best_features']\n",
    "    \n",
    "    test_classifier(\n",
    "        classifier, \n",
    "        data_dict, \n",
    "        feature_list=['poi'] + best_features\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[0]\n",
    "best_estimator = estimator['best_estimator_']\n",
    "best_features = ['poi'] + estimator['best_features']\n",
    "dump_classifier_and_data(best_estimator, data_dict, best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=5, score_func=<function f_classif at 0x7feb5dd634d0>)), ('classifier', GaussianNB(priors=None, var_smoothing=1.0))])\n",
      "\tAccuracy: 0.87571\tPrecision: 0.62264\tRecall: 0.33000\tF1: 0.43137\tF2: 0.36424\n",
      "\tTotal predictions: 14000\tTrue positives:  660\tFalse positives:  400\tFalse negatives: 1340\tTrue negatives: 11600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf, dataset, features_list = load_classifier_and_data()\n",
    "test_classifier(clf, dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f92b2924b84ff19c1c3dc485f7644d4486f64738191026bf8e6de303969141b5"
  },
  "kernelspec": {
   "display_name": "Python 2.7.18 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
